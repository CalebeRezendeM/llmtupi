{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM Training\n",
    "\n",
    "In this notebook we'll cover the training process for masked-language modeling (MLM). First we import and initialize everything required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mmaguero/gn-bert-small-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import csv, re, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = BertForMaskedLM.from_pretrained('mmaguero/gn-bert-small-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using *Meditations* by *Marcus Aurelius* as our training data. The file below has already been cleaned and so no further processing is required (beyond the `split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_tupi = lambda x: re.sub('\\s+', ' ', x.replace(\"'\",\"ç\"))\n",
    "with open('../nhe-enga/anotated_results.json', 'r') as fp:\n",
    "    text = [{\"input\":normalize_tupi(x['label']), 'output':normalize_tupi(x['anotated'])} for x in json.load(fp)]\n",
    "with open('../nhe-enga/anotated_tokens.json', 'r') as fp:\n",
    "    tokens = [normalize_tupi(x) for x in json.load(fp) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'xe oroeî',\n",
       "  'output': 'xe[SUBJECT:1ps] oro[OBJECT:2ps:SUBJECT_1P]eî[VERB]'},\n",
       " {'input': 'xe nçoroeî',\n",
       "  'output': 'xe[SUBJECT:1ps] nç[NEGATION_PREFIX]oro[OBJECT:2ps:SUBJECT_1P]e[VERB]î[NEGATION_SUFFIX:VOWEL_ENDING]'},\n",
       " {'input': 'xe opoeî',\n",
       "  'output': 'xe[SUBJECT:1ps] opo[OBJECT:2pp:SUBJECT_1P]eî[VERB]'},\n",
       " {'input': 'xe nçopoeî',\n",
       "  'output': 'xe[SUBJECT:1ps] nç[NEGATION_PREFIX]opo[OBJECT:2pp:SUBJECT_1P]e[VERB]î[NEGATION_SUFFIX:VOWEL_ENDING]'},\n",
       " {'input': 'ixé açe aîoseî',\n",
       "  'output': 'ixé[SUBJECT:1ps] açe[OBJECT:3p] a[SUBJECT_PREFIX:1ps]îos[OBJECT_MARKER:3p:PLURIFORM_PREFIX:MONOSYLLABIC]eî[VERB]'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = text\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield [x[\"output\"] for x in samples]\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': tokens, 'pad_token': '[PAD]'}\n",
    "num_added_toks = old_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll tokenize our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH=20\n",
    "# special_tokens_dict = {'additional_special_tokens': tokens}\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "inputs = tokenizer([x['input'] for x in text], return_tensors='pt', max_length=MAX_INPUT_LENGTH, truncation=True, padding='max_length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 18, 318,  16,   1,   7, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "        106, 106, 106, 106, 106, 106])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create our *labels* tensor by cloning the *input_ids* tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = tokenizer([x['output'] for x in text], return_tensors='pt', max_length=MAX_INPUT_LENGTH, truncation=True, padding='max_length').input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 18,  22, 318,  16,  93,   1,   7,  42, 106, 106, 106, 106, 106, 106,\n",
       "        106, 106, 106, 106, 106, 106])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mask tokens in the *input_ids* tensor, using the 15% probability we used before - and the **not** a *CLS* or *SEP* token condition. This time, because we have padding tokens we also need to exclude *PAD* tokens (*0* input ids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create random array of floats with equal dimensions to input_ids tensor\n",
    "# rand = torch.rand(inputs.input_ids.shape)\n",
    "# # create mask array\n",
    "# mask_arr = (rand < 0.15) * (inputs.input_ids != 2) * \\\n",
    "#            (inputs.input_ids != 3) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we take take the indices of each `True` value, within each individual vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection = []\n",
    "\n",
    "# for i in range(inputs.input_ids.shape[0]):\n",
    "#     selection.append(\n",
    "#         torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply these indices to each respective row in *input_ids*, assigning each of the values at these indices as *103*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(inputs.input_ids.shape[0]):\n",
    "#     inputs.input_ids[i, selection[i]] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the values *103* have been assigned in the same positions as we found *True* values in the `mask_arr` tensor.\n",
    "\n",
    "The `inputs` tensors are now ready, and can we can begin setting them up to be fed into our model during training. We create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationsDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(5247, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=5247, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kian/anaconda3/envs/jupyter/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for two epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                      | 0/14469 [00:00<?, ?it/s]/var/folders/b_/pp40hy517pdbcjp0b95t2ngc0000gn/T/ipykernel_31969/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0:  59%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                         | 8601/14469 [07:49<05:19, 18.34it/s, loss=0.0192]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print relevant info to progress bar\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go, we've fine-tuned our BERT model using MLM on *Meditations*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ixé îagûara aîuká\n",
      "ixé [SUBJECT:1ps] Ġ îa [SUBJECT_PREFIX:1ppi] g û a r a [VERB] a î î [VERB] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "ixé îagûara aîuká\n",
      "ixé [SUBJECT:1ps] Ġ îa [GERUND_SUBJECT_PREFIX:1ps] g û a r a a a î î [VERB] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "ixé îagûara aîuká\n",
      "ixé [SUBJECT:1ps] Ġ îa [GERUND_SUBJECT_PREFIX:1ps] g û a r a [VERB] a î î [VERB] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "ixé îagûara aîuká\n",
      "ixé [SUBJECT:1ps] Ġ îa [GERUND_SUBJECT_PREFIX:1ps] g û a r a a a î ukÃ¡ [VERB] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "ixé îagûara aîuká\n",
      "ixé [SUBJECT:1ps] Ġ îa [GERUND_SUBJECT_PREFIX:1ps] g û a r a [VERB] a î î [VERB] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # Now we will test the model with a sample sentence\n",
    "    sentence = normalize_tupi(\"Ixé îagûara Aîuká\").lower()\n",
    "    print(sentence)\n",
    "    # Now we will use this sentence to predict the masked word\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', max_length=MAX_INPUT_LENGTH, truncation=True, padding='max_length')\n",
    "    # print(inputs)\n",
    "    # # Now we will test the model with a sample sentence\n",
    "    # sentence = normalize_tupi(\"..[MASK] endé ruba. ixé nde ra'yra\").replace('[mask]', '[MASK]')\n",
    "    # # Now we will use this sentence to predict the masked word\n",
    "    # inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    # print(inputs)\n",
    "    inputs.to(device)\n",
    "    # get logits of the prediction\n",
    "    logits = model(**inputs).logits\n",
    "    # get index of most likely prediction\n",
    "    predicted_index = torch.argmax(logits[0, 3]).item()\n",
    "    # Get a string representation of the most likely word\n",
    "    pred = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    # print(pred)\n",
    "    # Now do rhat for all the predictions\n",
    "    predicted_indices = torch.argmax(logits, dim=2)\n",
    "    toks = tokenizer.convert_ids_to_tokens(predicted_indices[0])\n",
    "    # print(toks)\n",
    "    print(\" \".join(toks).replace(' ##', '').replace(' \\' ', \"'\").replace('ç', \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
